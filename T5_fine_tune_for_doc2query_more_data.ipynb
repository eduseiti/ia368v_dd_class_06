{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO+5/CIbSov976Q7d0ujAwW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eduseiti/ia368v_dd_class_06/blob/main/T5_fine_tune_for_doc2query_more_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This notebook executes the T5-base fine-tuning for doc2query task using a bigger training dataset â€• the MS MARCO passage development dataset"
      ],
      "metadata": {
        "id": "vlezFl33K6Vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q\n",
        "!pip install evaluate -q\n",
        "!pip install ftfy -q\n",
        "!pip install sentencepiece -q\n",
        "!pip install sacrebleu -q\n",
        "!pip install comet_ml -q"
      ],
      "metadata": {
        "id": "IyoODiJf7UDV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WORKING_FOLDER=\"drive/MyDrive/unicamp/ia368v_dd/aula_06\"\n",
        "\n",
        "API_KEYS_FILE=\"/content/drive/MyDrive/unicamp/ia368v_dd/api_keys_20230324.json\"\n",
        "\n",
        "TRAIN_OUTPUT_FOLDER=\"./trained_model_more_data\"\n",
        "MS_MARCO_SPLIT=\"ms_marco_passage_dev_data_split.pkl\"\n",
        "\n",
        "MS_MARCO_PASSAGE_DEV_TOKENIZED_DATASETS=\"ms_marco_passage_dev_tokenized_datasets.pkl\"\n",
        "\n",
        "MS_MARCO_PASSAGE_DEV_FILENAME=\"ms_marco_passage_dev.pkl\"\n",
        "\n",
        "LINK_WITH_COMET=True"
      ],
      "metadata": {
        "id": "gB0nBfaiLYJC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import json\n",
        "\n",
        "import ftfy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "import pickle\n",
        "\n",
        "if LINK_WITH_COMET:\n",
        "    from comet_ml import Experiment"
      ],
      "metadata": {
        "id": "J1H5lQXWbFF5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "os.chdir(WORKING_FOLDER)"
      ],
      "metadata": {
        "id": "mFNsI1WXLtsP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9077387a-4d4a-421f-fca0-c75e5748a32e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link with comet-ml"
      ],
      "metadata": {
        "id": "YcuHmBPDaDLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if LINK_WITH_COMET:\n",
        "    with open(API_KEYS_FILE) as inputFile:\n",
        "        api_keys = json.load(inputFile)\n",
        "\n",
        "    os.environ[\"COMET_API_KEY\"] = api_keys['comet_ml']\n",
        "    os.environ[\"COMET_LOG_ASSETS\"] = \"True\"\n",
        "    os.environ['COMET_MODE'] = \"ONLINE\"\n",
        "\n",
        "    experiment = Experiment(api_key=api_keys['comet_ml'], \n",
        "                            project_name=\"causal-language-model-fine-tuning\",\n",
        "                            workspace=\"eduseiti\")    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXxEFPRTaBf4",
        "outputId": "3079252f-12a1-4328-e443-bcc76ace9e72"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "COMET ERROR: Failed to calculate active processors count. Fall back to default CPU count 1\n",
            "COMET INFO: Couldn't find a Git repository in '/content/drive/MyDrive/unicamp/ia368v_dd/aula_06' nor in any parent directory. You can override where Comet is looking for a Git Patch by setting the configuration `COMET_GIT_DIRECTORY`\n",
            "COMET INFO: Experiment is live on comet.com https://www.comet.com/eduseiti/causal-language-model-fine-tuning/e689765b91c540b8b4dc5fd5e6bf155a\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (AutoTokenizer, \n",
        "                          AutoModelForSeq2SeqLM, \n",
        "                          Seq2SeqTrainer,\n",
        "                          Seq2SeqTrainingArguments,\n",
        "                          TrainerCallback, \n",
        "                          get_cosine_with_hard_restarts_schedule_with_warmup,\n",
        "                          DataCollatorForSeq2Seq,\n",
        "                          T5Tokenizer, \n",
        "                          T5Model\n",
        "                          )\n",
        "\n",
        "import torch\n",
        "\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "4IGkkxJCjxef"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)"
      ],
      "metadata": {
        "id": "wgP6ie7r_uqD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the MS MARCO passage dev data split, if available"
      ],
      "metadata": {
        "id": "1Gtj85x8yiCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Doc2queryFinetuning(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, ms_df, tokenizer):\n",
        "\n",
        "        self.tokenized_topics = tokenizer(ms_df['query_text'].tolist(), return_length=True)\n",
        "        self.tokenized_passage = tokenizer(ms_df['passage_text'].tolist(), return_length=True)\n",
        "\n",
        "        print(\"Queries tokens size stats:\\n{}\\n\".format(stats.describe(self.tokenized_topics['length'])))\n",
        "        print(\"Passages tokens size stats:\\n{}\\n\".format(stats.describe(self.tokenized_passage['length'])))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_topics['input_ids'])\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return {'input_ids': self.tokenized_passage['input_ids'][index],\n",
        "                'attention_mask': self.tokenized_passage['attention_mask'][index],\n",
        "                'labels': self.tokenized_topics['input_ids'][index]}"
      ],
      "metadata": {
        "id": "4X2_jX_tw3Vr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTrainerCallback(TrainerCallback):\n",
        "\n",
        "    def __init__(self, best_validation_yet=99999, model=None) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.best_validation_metric = best_validation_yet\n",
        "        self.model = model\n",
        "\n",
        "\n",
        "    def on_evaluate(self, args, state, control, model=None, metrics=None, **kwargs):\n",
        "        print(metrics.keys())\n",
        "\n",
        "        print(\"metrics['eval_loss']={}\".format(metrics['eval_loss']))\n",
        "        print(\"metrics['eval_bleu']={}\".format(metrics['eval_bleu']))\n",
        "\n",
        "\n",
        "        if metrics['eval_bleu'] > self.best_validation_metric:\n",
        "            self.model.save_pretrained(os.path.join(TRAIN_OUTPUT_FOLDER, \n",
        "                                                    \"checkpoint-{}-{:.4f}\".format(state.global_step,\n",
        "                                                                                  metrics['eval_bleu'])))\n",
        "            self.best_validation_metric = metrics['eval_bleu']"
      ],
      "metadata": {
        "id": "xiR4Dm69w3QB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(MS_MARCO_PASSAGE_DEV_TOKENIZED_DATASETS):\n",
        "    with open(MS_MARCO_PASSAGE_DEV_TOKENIZED_DATASETS, \"rb\") as inputFile:\n",
        "        ms_marco_datasets = pickle.load(inputFile)\n",
        "\n",
        "    train_dataset = ms_marco_datasets['train_dataset']\n",
        "    eval_dataset = ms_marco_datasets['eval_dataset']\n",
        "else:\n",
        "    print(\"Need to import and fix the training dataset...\")"
      ],
      "metadata": {
        "id": "g_XrVNA7yl5-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the preprocessed data\n",
        "\n",
        "The data should have been preprocessed by the [`explore_ms_marco_passage.ipynb`](explore_ms_marco_passage.ipynb) notebook."
      ],
      "metadata": {
        "id": "oR10gOwEh83h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not 'train_dataset' in locals():\n",
        "    with open(MS_MARCO_PASSAGE_DEV_FILENAME, 'rb') as inputFile:\n",
        "        ms_df = pickle.load(inputFile)\n",
        "\n",
        "        ms_df['query_text'] = ms_df['query_text'].apply(lambda text: ftfy.fix_text(text))\n",
        "        ms_df['passage_text'] = ms_df['passage_text'].apply(lambda text: ftfy.fix_text(text))\n",
        "\n",
        "else:\n",
        "    print(\"Datasets have already been loaded...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEB5xlBIYL1Q",
        "outputId": "61d3246d-9951-43d6-c875-edcc27efafe6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets have already been loaded...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split evaluation part"
      ],
      "metadata": {
        "id": "DvWpFpGpwaFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not 'train_dataset' in locals():\n",
        "    print(\"ms_df.shape={}\".format(ms_df.shape))\n",
        "\n",
        "    split_entries = np.random.choice(list(range(ms_df.shape[0])), 1000, replace=False)\n",
        "\n",
        "    train_df = ms_df.iloc[np.setdiff1d(list(range(ms_df.shape[0])), split_entries)].reset_index(drop=True)\n",
        "    validation_df = ms_df.iloc[split_entries].reset_index(drop=True)\n",
        "    \n",
        "    print(\"train_df.shape={}\".format(train_df.shape))\n",
        "    print(\"validation_df.shape={}\".format(validation_df.shape))\n",
        "    \n",
        "    with open(MS_MARCO_SPLIT, \"wb\") as outputFile:\n",
        "        pickle.dump({'train': train_df, \n",
        "                     'validation': validation_df}, outputFile, pickle.HIGHEST_PROTOCOL)\n",
        "        \n",
        "else:\n",
        "    print(\"Datasets have already been loaded...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVj26xQNwaBm",
        "outputId": "a1d4c45c-9d2c-434f-aea9-2159291c2da3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets have already been loaded...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine tune the T5-base model for the query generation"
      ],
      "metadata": {
        "id": "ONzoumZjc5W6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare T5 model"
      ],
      "metadata": {
        "id": "Cyv1yI_xw2hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhGbxDBGkCXf",
        "outputId": "3337da25-2ef5-4e62-b44a-1c977527a466"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model_max_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpe6QbnI6a81",
        "outputId": "edfb8515-6ef7-4743-e902-78881741269c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")"
      ],
      "metadata": {
        "id": "L3YKKvEUlpkM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create and save the datasets, if they are not ready yet..."
      ],
      "metadata": {
        "id": "Cqq9J6BkzqUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not 'train_dataset' in locals():\n",
        "    train_dataset = Doc2queryFinetuning(train_df, tokenizer)\n",
        "    eval_dataset = Doc2queryFinetuning(validation_df, tokenizer)\n",
        "\n",
        "    with open(MS_MARCO_PASSAGE_DEV_TOKENIZED_DATASETS, 'wb') as outputFile:\n",
        "        pickle.dump({\"train_dataset\": train_dataset,\n",
        "                     \"eval_dataset\": eval_dataset}, outputFile, pickle.HIGHEST_PROTOCOL)    \n",
        "else:\n",
        "    print(\"Datasets have already been loaded...\")"
      ],
      "metadata": {
        "id": "kX75h3oCw3Kw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1b66c7a-5e1b-492f-be62-4639ded59085"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datasets have already been loaded...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This part was taken from the [`run_translation.py`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/translation/run_translation.py) script."
      ],
      "metadata": {
        "id": "Y9VXQsi5BBWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"sacrebleu\")"
      ],
      "metadata": {
        "id": "xRC29Hizk56U"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels"
      ],
      "metadata": {
        "id": "Nj8a-g2jBy5G"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_preds):\n",
        "\n",
        "    preds, labels = eval_preds\n",
        "\n",
        "    # print(\"len(preds)={}\".format(len(preds)))\n",
        "\n",
        "    # for i in range(len(preds)):\n",
        "    #     print(\"len(preds[{}])={}\".format(i, len(preds[i])))\n",
        "    #     print(\"preds[{}].shape={}\".format(i, preds[i].shape))\n",
        "    #     print(\"preds[i]={}\".format(preds[i]))\n",
        "\n",
        "\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    print(\"compute_metrics. preds.shape={}\".format(preds.shape))\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    \n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    \n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "doB4B3SVA4s_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=22\n",
        "gradient_accumulation_steps=8\n",
        "epochs=10"
      ],
      "metadata": {
        "id": "kT5rUueI658D"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_params = Seq2SeqTrainingArguments(output_dir=TRAIN_OUTPUT_FOLDER,\n",
        "                                           num_train_epochs=epochs,\n",
        "                                           per_device_train_batch_size=batch_size,\n",
        "                                           per_device_eval_batch_size=batch_size,\n",
        "                                           gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "                                           evaluation_strategy='steps',\n",
        "                                           eval_steps=50,\n",
        "                                           save_strategy='steps',\n",
        "                                           save_steps=1000,\n",
        "                                           logging_strategy='steps',\n",
        "                                           logging_steps=10,\n",
        "                                           save_total_limit=10,\n",
        "                                           # report_to='comet_ml',\n",
        "                                           # dataloader_num_workers=2,\n",
        "                                           dataloader_pin_memory=True,\n",
        "                                           predict_with_generate=True,\n",
        "                                           generation_num_beams=10,\n",
        "                                           fp16=True)"
      ],
      "metadata": {
        "id": "99L0Mq_p7s4M"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_pad_token_id = -100\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=label_pad_token_id,\n",
        "    pad_to_multiple_of=8 if training_params.fp16 else None,\n",
        ")"
      ],
      "metadata": {
        "id": "FsmNhge3AIRP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_callback = CustomTrainerCallback(best_validation_yet=-1, \n",
        "                                         model=model)"
      ],
      "metadata": {
        "id": "phSscTZI3hO4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_training_steps = epochs * int(len(train_dataset) // (batch_size * gradient_accumulation_steps))\n",
        "\n",
        "optimzer = torch.optim.AdamW(model.parameters(), lr=6e-4, weight_decay=1e-3)\n",
        "scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimzer, \n",
        "                                                               0,\n",
        "                                                               num_training_steps, \n",
        "                                                               num_cycles=25)"
      ],
      "metadata": {
        "id": "8y9rWOcRbZoV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_training_steps // 25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMDCHMew-9IL",
        "outputId": "fdff50fe-52b4-4bfe-f7ae-01ca284fb654"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1208"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(model=model,\n",
        "                         args=training_params,\n",
        "                         train_dataset=train_dataset,\n",
        "                         eval_dataset=eval_dataset,\n",
        "                         data_collator=data_collator,\n",
        "                         callbacks=[trainer_callback],\n",
        "                         optimizers=(optimzer, scheduler),\n",
        "                         tokenizer=tokenizer,\n",
        "                         compute_metrics=compute_metrics\n",
        "                         )"
      ],
      "metadata": {
        "id": "77Sgz4OB8nMp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_result = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mefCKkW60Ko0",
        "outputId": "3c78482e-4e35-446e-9f3f-729bd57dd599"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "COMET INFO: ---------------------------------------------------------------------------------------\n",
            "COMET INFO: Comet.ml Experiment Summary\n",
            "COMET INFO: ---------------------------------------------------------------------------------------\n",
            "COMET INFO:   Data:\n",
            "COMET INFO:     display_summary_level : 1\n",
            "COMET INFO:     url                   : https://www.comet.com/eduseiti/causal-language-model-fine-tuning/e689765b91c540b8b4dc5fd5e6bf155a\n",
            "COMET INFO:   Uploads:\n",
            "COMET INFO:     environment details : 1\n",
            "COMET INFO:     filename            : 1\n",
            "COMET INFO:     installed packages  : 1\n",
            "COMET INFO:     notebook            : 2\n",
            "COMET INFO:     os packages         : 1\n",
            "COMET INFO:     source_code         : 1\n",
            "COMET INFO: \n",
            "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "COMET ERROR: Failed to calculate active processors count. Fall back to default CPU count 1\n",
            "COMET INFO: Couldn't find a Git repository in '/content/drive/MyDrive/unicamp/ia368v_dd/aula_06' nor in any parent directory. You can override where Comet is looking for a Git Patch by setting the configuration `COMET_GIT_DIRECTORY`\n",
            "COMET INFO: Experiment is live on comet.com https://www.comet.com/eduseiti/huggingface/8e97bfc47ec84b8db5b8519354a5c1e7\n",
            "\n",
            "COMET ERROR: Failed to extract scalar from SummaryWriter.add_hparams()\n",
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='179' max='30210' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  179/30210 13:37 < 38:32:23, 0.22 it/s, Epoch 0.06/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.103600</td>\n",
              "      <td>1.672017</td>\n",
              "      <td>16.558900</td>\n",
              "      <td>9.533000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.755200</td>\n",
              "      <td>1.609427</td>\n",
              "      <td>17.352600</td>\n",
              "      <td>9.649000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.733200</td>\n",
              "      <td>1.571516</td>\n",
              "      <td>17.830600</td>\n",
              "      <td>9.463000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.6720167398452759\n",
            "metrics['eval_bleu']=16.5589\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.609426736831665\n",
            "metrics['eval_bleu']=17.3526\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.5715161561965942\n",
            "metrics['eval_bleu']=17.8306\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-a6eca412ee9e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         )\n\u001b[0;32m-> 1633\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1634\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1900\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1901\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1902\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2653\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2654\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_grad_scaling\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2655\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2656\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_apex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2657\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mscaled_loss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/comet_ml/monkey_patching.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m                     )\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0mreturn_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0;31m# Call after callbacks once we have the return value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "experiment.end()"
      ],
      "metadata": {
        "id": "pueVwXfaaydQ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zTOeWzzZgNsr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}