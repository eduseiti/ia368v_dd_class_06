{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMQF9HopTn3vkVNomHJB5vm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eduseiti/ia368v_dd_class_06/blob/main/T5_fine_tune_for_doc2query.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q\n",
        "!pip install evaluate -q\n",
        "!pip install ftfy -q\n",
        "!pip install sentencepiece -q\n",
        "!pip install sacrebleu -q\n",
        "!pip install comet_ml -q"
      ],
      "metadata": {
        "id": "IyoODiJf7UDV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "WORKING_FOLDER=\"drive/MyDrive/unicamp/ia368v_dd/aula_06\"\n",
        "\n",
        "API_KEYS_FILE=\"/content/drive/MyDrive/unicamp/ia368v_dd/api_keys_20230324.json\"\n",
        "\n",
        "TRAIN_OUTPUT_FOLDER=\"./trained_model\"\n",
        "MS_MARCO_SPLIT=\"ms_marco_tiny_data_split.pkl\"\n",
        "\n",
        "MS_MARCO_TINY_URL=\"https://storage.googleapis.com/unicamp-dl/ia368dd_2023s1/msmarco/msmarco_triples.train.tiny.tsv\"\n",
        "\n",
        "LINK_WITH_COMET=True"
      ],
      "metadata": {
        "id": "gB0nBfaiLYJC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import json\n",
        "\n",
        "import ftfy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "import pickle\n",
        "\n",
        "if LINK_WITH_COMET:\n",
        "    from comet_ml import Experiment"
      ],
      "metadata": {
        "id": "J1H5lQXWbFF5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "os.chdir(WORKING_FOLDER)"
      ],
      "metadata": {
        "id": "mFNsI1WXLtsP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56f448e7-fb46-4012-e0c0-81f5ef36bdd7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link with comet-ml"
      ],
      "metadata": {
        "id": "YcuHmBPDaDLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if LINK_WITH_COMET:\n",
        "    with open(API_KEYS_FILE) as inputFile:\n",
        "        api_keys = json.load(inputFile)\n",
        "\n",
        "    os.environ[\"COMET_API_KEY\"] = api_keys['comet_ml']\n",
        "    os.environ[\"COMET_LOG_ASSETS\"] = \"True\"\n",
        "    os.environ['COMET_MODE'] = \"ONLINE\"\n",
        "\n",
        "    experiment = Experiment(api_key=api_keys['comet_ml'], \n",
        "                            project_name=\"causal-language-model-fine-tuning\",\n",
        "                            workspace=\"eduseiti\")    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXxEFPRTaBf4",
        "outputId": "b86fe72f-5e71-4e25-d2c3-22638da7dd58"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "COMET ERROR: Failed to calculate active processors count. Fall back to default CPU count 1\n",
            "COMET INFO: Couldn't find a Git repository in '/content/drive/MyDrive/unicamp/ia368v_dd/aula_06' nor in any parent directory. You can override where Comet is looking for a Git Patch by setting the configuration `COMET_GIT_DIRECTORY`\n",
            "COMET INFO: Experiment is live on comet.com https://www.comet.com/eduseiti/causal-language-model-fine-tuning/950840124a174b7ea97e635140f60242\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (AutoTokenizer, \n",
        "                          AutoModelForSeq2SeqLM, \n",
        "                          Seq2SeqTrainer,\n",
        "                          Seq2SeqTrainingArguments,\n",
        "                          TrainerCallback, \n",
        "                          get_cosine_with_hard_restarts_schedule_with_warmup,\n",
        "                          DataCollatorForSeq2Seq,\n",
        "                          T5Tokenizer, \n",
        "                          T5Model\n",
        "                          )\n",
        "\n",
        "import torch\n",
        "\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "4IGkkxJCjxef"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)"
      ],
      "metadata": {
        "id": "wgP6ie7r_uqD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the MS MARCO data split, if available"
      ],
      "metadata": {
        "id": "1Gtj85x8yiCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(MS_MARCO_SPLIT):\n",
        "    with open(MS_MARCO_SPLIT, \"rb\") as inputFile:\n",
        "        ms_marco_data = pickle.load(inputFile)\n",
        "\n",
        "    train_df = ms_marco_data['train']\n",
        "    validation_df = ms_marco_data['validation']\n",
        "else:\n",
        "    print(\"Need to import and fix the training dataset...\")"
      ],
      "metadata": {
        "id": "g_XrVNA7yl5-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import and fix training dataset"
      ],
      "metadata": {
        "id": "oR10gOwEh83h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not 'train_df' in locals():\n",
        "    if not os.path.exists(os.path.basename(MS_MARCO_TINY_URL)):\n",
        "        !wget {MS_MARCO_TINY_URL}\n",
        "    else:\n",
        "        print(\"Training dataset already downloaded...\")\n",
        "\n",
        "    ms_df = pd.read_csv(os.path.basename(MS_MARCO_TINY_URL), sep=\"\\t\", header=None, names=['topic', 'positive', 'negative'])\n",
        "    pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "    display(ms_df.head())\n",
        "\n",
        "    ms_df['positive'] = ms_df['positive'].apply(lambda text: ftfy.fix_text(text))\n",
        "    ms_df = ms_df.drop('negative', axis=1)\n",
        "\n",
        "else:\n",
        "    print(\"Data split has already been loaded...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEB5xlBIYL1Q",
        "outputId": "ffccdb89-266a-421b-e9dd-703f29285afc"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data split has already been loaded...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split evaluation part"
      ],
      "metadata": {
        "id": "DvWpFpGpwaFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not 'train_df' in locals():\n",
        "    print(\"ms_df.shape={}\".format(ms_df.shape))\n",
        "\n",
        "    split_entries = np.random.choice(list(range(ms_df.shape[0])), 1000, replace=False)\n",
        "\n",
        "    train_df = ms_df.iloc[np.setdiff1d(list(range(ms_df.shape[0])), split_entries)].reset_index(drop=True)\n",
        "    validation_df = ms_df.iloc[split_entries].reset_index(drop=True)\n",
        "    \n",
        "    print(\"train_df.shape={}\".format(train_df.shape))\n",
        "    print(\"validation_df.shape={}\".format(validation_df.shape))\n",
        "    \n",
        "    with open(MS_MARCO_SPLIT, \"wb\") as outputFile:\n",
        "        pickle.dump({'train': train_df, \n",
        "                    'validation': validation_df}, outputFile, pickle.HIGHEST_PROTOCOL)\n",
        "        \n",
        "else:\n",
        "    print(\"Data split has already been loaded...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVj26xQNwaBm",
        "outputId": "9c5f5b8c-41a3-4f43-844c-8ae86968f50d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data split has already been loaded...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine tune the T5-base model for the query generation"
      ],
      "metadata": {
        "id": "ONzoumZjc5W6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Doc2queryFinetuning(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, ms_df, tokenizer):\n",
        "\n",
        "        self.tokenized_topics = tokenizer(ms_df['topic'].tolist(), return_length=True)\n",
        "        self.tokenized_passage = tokenizer(ms_df['positive'].tolist(), return_length=True)\n",
        "\n",
        "        print(\"Topics tokens size stats:\\n{}\\n\".format(stats.describe(self.tokenized_topics['length'])))\n",
        "        print(\"Passages tokens size stats:\\n{}\\n\".format(stats.describe(self.tokenized_passage['length'])))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_topics['input_ids'])\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return {'input_ids': self.tokenized_passage['input_ids'][index],\n",
        "                'attention_mask': self.tokenized_passage['attention_mask'][index],\n",
        "                'labels': self.tokenized_topics['input_ids'][index]}"
      ],
      "metadata": {
        "id": "4X2_jX_tw3Vr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTrainerCallback(TrainerCallback):\n",
        "\n",
        "    def __init__(self, best_validation_yet=99999, model=None) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.best_validation_metric = best_validation_yet\n",
        "        self.model = model\n",
        "\n",
        "\n",
        "    def on_evaluate(self, args, state, control, model=None, metrics=None, **kwargs):\n",
        "        print(metrics.keys())\n",
        "\n",
        "        print(\"metrics['eval_loss']={}\".format(metrics['eval_loss']))\n",
        "        print(\"metrics['eval_bleu']={}\".format(metrics['eval_bleu']))\n",
        "\n",
        "\n",
        "        if metrics['eval_bleu'] > self.best_validation_metric:\n",
        "            self.model.save_pretrained(os.path.join(TRAIN_OUTPUT_FOLDER, \n",
        "                                                    \"checkpoint-{}-{:.4f}\".format(state.global_step,\n",
        "                                                                                  metrics['eval_bleu'])))\n",
        "            self.best_validation_metric = metrics['eval_bleu']"
      ],
      "metadata": {
        "id": "xiR4Dm69w3QB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare T5 model"
      ],
      "metadata": {
        "id": "Cyv1yI_xw2hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhGbxDBGkCXf",
        "outputId": "93c59a0f-ee85-4796-d82e-78556e0d8501"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model_max_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpe6QbnI6a81",
        "outputId": "47034e83-c84a-4bc5-a34d-231c242039f4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")"
      ],
      "metadata": {
        "id": "L3YKKvEUlpkM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the datasets"
      ],
      "metadata": {
        "id": "Cqq9J6BkzqUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Doc2queryFinetuning(train_df, tokenizer)\n",
        "eval_dataset = Doc2queryFinetuning(validation_df, tokenizer)"
      ],
      "metadata": {
        "id": "kX75h3oCw3Kw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6a668a3-c8cb-4444-9266-88d64b2e4f3e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topics tokens size stats:\n",
            "DescribeResult(nobs=10000, minmax=(3, 57), mean=9.5105, variance=11.948684618461847, skewness=1.7547439898740915, kurtosis=10.741828171750589)\n",
            "\n",
            "Passages tokens size stats:\n",
            "DescribeResult(nobs=10000, minmax=(14, 326), mean=86.9099, variance=1264.7692589158914, skewness=1.140892273381897, kurtosis=1.6057605062545974)\n",
            "\n",
            "Topics tokens size stats:\n",
            "DescribeResult(nobs=1000, minmax=(3, 29), mean=9.434, variance=11.218862862862863, skewness=1.3393629730350802, kurtosis=4.007153813445774)\n",
            "\n",
            "Passages tokens size stats:\n",
            "DescribeResult(nobs=1000, minmax=(16, 261), mean=87.08, variance=1265.3289289289291, skewness=1.1804618158884206, kurtosis=1.6154309574375878)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This part was taken from the [`run_translation.py`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/translation/run_translation.py) script."
      ],
      "metadata": {
        "id": "Y9VXQsi5BBWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"sacrebleu\")"
      ],
      "metadata": {
        "id": "xRC29Hizk56U"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels"
      ],
      "metadata": {
        "id": "Nj8a-g2jBy5G"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_preds):\n",
        "\n",
        "    preds, labels = eval_preds\n",
        "\n",
        "    # print(\"len(preds)={}\".format(len(preds)))\n",
        "\n",
        "    # for i in range(len(preds)):\n",
        "    #     print(\"len(preds[{}])={}\".format(i, len(preds[i])))\n",
        "    #     print(\"preds[{}].shape={}\".format(i, preds[i].shape))\n",
        "    #     print(\"preds[i]={}\".format(preds[i]))\n",
        "\n",
        "\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    print(\"compute_metrics. preds.shape={}\".format(preds.shape))\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    \n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    \n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "doB4B3SVA4s_"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=26\n",
        "gradient_accumulation_steps=8\n",
        "epochs=100"
      ],
      "metadata": {
        "id": "kT5rUueI658D"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_params = Seq2SeqTrainingArguments(output_dir=TRAIN_OUTPUT_FOLDER,\n",
        "                                           num_train_epochs=epochs,\n",
        "                                           per_device_train_batch_size=batch_size,\n",
        "                                           per_device_eval_batch_size=batch_size,\n",
        "                                           gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "                                           evaluation_strategy='steps',\n",
        "                                           eval_steps=50,\n",
        "                                           save_strategy='steps',\n",
        "                                           save_steps=1000,\n",
        "                                           logging_strategy='steps',\n",
        "                                           logging_steps=10,\n",
        "                                           save_total_limit=10,\n",
        "                                           # report_to='comet_ml',\n",
        "                                           # dataloader_num_workers=2,\n",
        "                                           dataloader_pin_memory=True,\n",
        "                                           predict_with_generate=True,\n",
        "                                           fp16=True)"
      ],
      "metadata": {
        "id": "99L0Mq_p7s4M"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_pad_token_id = -100\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=label_pad_token_id,\n",
        "    pad_to_multiple_of=8 if training_params.fp16 else None,\n",
        ")"
      ],
      "metadata": {
        "id": "FsmNhge3AIRP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_callback = CustomTrainerCallback(best_validation_yet=-1, \n",
        "                                         model=model)"
      ],
      "metadata": {
        "id": "phSscTZI3hO4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_training_steps = epochs * int(len(train_dataset) // (batch_size * gradient_accumulation_steps))\n",
        "\n",
        "optimzer = torch.optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-3)\n",
        "scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimzer, \n",
        "                                                               0,\n",
        "                                                               num_training_steps, \n",
        "                                                               num_cycles=10)"
      ],
      "metadata": {
        "id": "8y9rWOcRbZoV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_training_steps // 10"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMDCHMew-9IL",
        "outputId": "42542830-43e2-4f09-921f-ad966f250ef6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "480"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(model=model,\n",
        "                         args=training_params,\n",
        "                         train_dataset=train_dataset,\n",
        "                         eval_dataset=eval_dataset,\n",
        "                         data_collator=data_collator,\n",
        "                         callbacks=[trainer_callback],\n",
        "                         optimizers=(optimzer, scheduler),\n",
        "                         tokenizer=tokenizer,\n",
        "                         compute_metrics=compute_metrics\n",
        "                         )"
      ],
      "metadata": {
        "id": "77Sgz4OB8nMp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_result = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mefCKkW60Ko0",
        "outputId": "01e6b9c9-04e9-4635-fee0-20bab7bb7af4"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "COMET INFO: ---------------------------------------------------------------------------------------\n",
            "COMET INFO: Comet.ml Experiment Summary\n",
            "COMET INFO: ---------------------------------------------------------------------------------------\n",
            "COMET INFO:   Data:\n",
            "COMET INFO:     display_summary_level : 1\n",
            "COMET INFO:     url                   : https://www.comet.com/eduseiti/causal-language-model-fine-tuning/950840124a174b7ea97e635140f60242\n",
            "COMET INFO:   Uploads:\n",
            "COMET INFO:     environment details : 1\n",
            "COMET INFO:     filename            : 1\n",
            "COMET INFO:     installed packages  : 1\n",
            "COMET INFO:     notebook            : 2\n",
            "COMET INFO:     os packages         : 1\n",
            "COMET INFO:     source_code         : 1\n",
            "COMET INFO: \n",
            "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "COMET ERROR: Failed to calculate active processors count. Fall back to default CPU count 1\n",
            "COMET INFO: Couldn't find a Git repository in '/content/drive/MyDrive/unicamp/ia368v_dd/aula_06' nor in any parent directory. You can override where Comet is looking for a Git Patch by setting the configuration `COMET_GIT_DIRECTORY`\n",
            "COMET INFO: Experiment is live on comet.com https://www.comet.com/eduseiti/huggingface/77a26aa5b0cb4245834c8fbb0a20260b\n",
            "\n",
            "COMET ERROR: Failed to extract scalar from SummaryWriter.add_hparams()\n",
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3710' max='4800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3710/4800 3:46:55 < 1:06:42, 0.27 it/s, Epoch 77.07/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>2.014100</td>\n",
              "      <td>1.591691</td>\n",
              "      <td>16.088700</td>\n",
              "      <td>9.099000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.518400</td>\n",
              "      <td>1.530510</td>\n",
              "      <td>17.872600</td>\n",
              "      <td>8.823000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>1.489600</td>\n",
              "      <td>1.520247</td>\n",
              "      <td>17.842800</td>\n",
              "      <td>9.158000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.220100</td>\n",
              "      <td>1.524088</td>\n",
              "      <td>18.493300</td>\n",
              "      <td>9.013000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>1.153900</td>\n",
              "      <td>1.541452</td>\n",
              "      <td>18.977200</td>\n",
              "      <td>9.080000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.074500</td>\n",
              "      <td>1.557765</td>\n",
              "      <td>18.546500</td>\n",
              "      <td>8.982000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>1.193000</td>\n",
              "      <td>1.557839</td>\n",
              "      <td>18.827300</td>\n",
              "      <td>9.136000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.027600</td>\n",
              "      <td>1.568631</td>\n",
              "      <td>18.677000</td>\n",
              "      <td>9.139000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>1.074200</td>\n",
              "      <td>1.568504</td>\n",
              "      <td>18.729000</td>\n",
              "      <td>9.118000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.182700</td>\n",
              "      <td>1.530010</td>\n",
              "      <td>18.503100</td>\n",
              "      <td>9.246000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>1.143600</td>\n",
              "      <td>1.548131</td>\n",
              "      <td>17.371500</td>\n",
              "      <td>9.144000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.439900</td>\n",
              "      <td>1.743204</td>\n",
              "      <td>14.151900</td>\n",
              "      <td>8.628000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>1.795200</td>\n",
              "      <td>1.931029</td>\n",
              "      <td>12.285000</td>\n",
              "      <td>8.092000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.756300</td>\n",
              "      <td>1.877942</td>\n",
              "      <td>12.216600</td>\n",
              "      <td>8.420000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>1.807800</td>\n",
              "      <td>1.842948</td>\n",
              "      <td>12.668800</td>\n",
              "      <td>8.561000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.803600</td>\n",
              "      <td>1.831809</td>\n",
              "      <td>12.782100</td>\n",
              "      <td>8.569000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>1.665300</td>\n",
              "      <td>1.830179</td>\n",
              "      <td>12.778000</td>\n",
              "      <td>8.540000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.659300</td>\n",
              "      <td>1.828871</td>\n",
              "      <td>12.788200</td>\n",
              "      <td>8.545000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>1.670800</td>\n",
              "      <td>1.828547</td>\n",
              "      <td>12.832200</td>\n",
              "      <td>8.543000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.641300</td>\n",
              "      <td>1.827606</td>\n",
              "      <td>12.917900</td>\n",
              "      <td>8.560000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>1.642200</td>\n",
              "      <td>1.825544</td>\n",
              "      <td>13.117400</td>\n",
              "      <td>8.588000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>1.630500</td>\n",
              "      <td>1.826997</td>\n",
              "      <td>12.953800</td>\n",
              "      <td>8.606000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>1.636100</td>\n",
              "      <td>1.839826</td>\n",
              "      <td>12.941600</td>\n",
              "      <td>8.613000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.744200</td>\n",
              "      <td>1.848160</td>\n",
              "      <td>13.023500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>1.633200</td>\n",
              "      <td>1.848364</td>\n",
              "      <td>13.023500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>1.637500</td>\n",
              "      <td>1.848405</td>\n",
              "      <td>13.023500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>1.663300</td>\n",
              "      <td>1.848421</td>\n",
              "      <td>13.023500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.600100</td>\n",
              "      <td>1.848358</td>\n",
              "      <td>13.023500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>1.607800</td>\n",
              "      <td>1.848358</td>\n",
              "      <td>13.023500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.624500</td>\n",
              "      <td>1.848369</td>\n",
              "      <td>13.023500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>1.634600</td>\n",
              "      <td>1.848323</td>\n",
              "      <td>13.023500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.628500</td>\n",
              "      <td>1.848328</td>\n",
              "      <td>13.023500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>1.609800</td>\n",
              "      <td>1.848316</td>\n",
              "      <td>13.023500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>1.628400</td>\n",
              "      <td>1.848401</td>\n",
              "      <td>13.023500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>1.639600</td>\n",
              "      <td>1.848406</td>\n",
              "      <td>13.023500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>1.614500</td>\n",
              "      <td>1.848364</td>\n",
              "      <td>13.023500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>1.748100</td>\n",
              "      <td>1.848330</td>\n",
              "      <td>13.023500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>1.637600</td>\n",
              "      <td>1.848330</td>\n",
              "      <td>13.023500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>1.606300</td>\n",
              "      <td>1.848330</td>\n",
              "      <td>13.023500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.624700</td>\n",
              "      <td>1.848304</td>\n",
              "      <td>13.023500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>1.624600</td>\n",
              "      <td>1.848263</td>\n",
              "      <td>13.023500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>1.639600</td>\n",
              "      <td>1.848255</td>\n",
              "      <td>13.023500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>1.661500</td>\n",
              "      <td>1.848250</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>1.608300</td>\n",
              "      <td>1.848146</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>1.634400</td>\n",
              "      <td>1.848124</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>1.617200</td>\n",
              "      <td>1.848151</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>1.715800</td>\n",
              "      <td>1.848148</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>1.773700</td>\n",
              "      <td>1.848148</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>1.617200</td>\n",
              "      <td>1.848148</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>1.642300</td>\n",
              "      <td>1.848162</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>1.615800</td>\n",
              "      <td>1.848154</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>1.635300</td>\n",
              "      <td>1.848147</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2650</td>\n",
              "      <td>1.649100</td>\n",
              "      <td>1.848065</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>1.617700</td>\n",
              "      <td>1.848001</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2750</td>\n",
              "      <td>1.606300</td>\n",
              "      <td>1.848057</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2800</td>\n",
              "      <td>1.605200</td>\n",
              "      <td>1.848182</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2850</td>\n",
              "      <td>1.617000</td>\n",
              "      <td>1.848190</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2900</td>\n",
              "      <td>1.626700</td>\n",
              "      <td>1.848190</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2950</td>\n",
              "      <td>1.605600</td>\n",
              "      <td>1.848173</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>1.606800</td>\n",
              "      <td>1.848168</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3050</td>\n",
              "      <td>1.597900</td>\n",
              "      <td>1.848161</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3100</td>\n",
              "      <td>1.625500</td>\n",
              "      <td>1.848102</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3150</td>\n",
              "      <td>1.587700</td>\n",
              "      <td>1.848074</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3200</td>\n",
              "      <td>1.618800</td>\n",
              "      <td>1.848129</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3250</td>\n",
              "      <td>1.607700</td>\n",
              "      <td>1.848063</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3300</td>\n",
              "      <td>1.605800</td>\n",
              "      <td>1.848027</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3350</td>\n",
              "      <td>1.625200</td>\n",
              "      <td>1.848063</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3400</td>\n",
              "      <td>1.626300</td>\n",
              "      <td>1.848063</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3450</td>\n",
              "      <td>1.609900</td>\n",
              "      <td>1.848096</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>1.604400</td>\n",
              "      <td>1.848046</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3550</td>\n",
              "      <td>1.618600</td>\n",
              "      <td>1.848024</td>\n",
              "      <td>13.009500</td>\n",
              "      <td>8.639000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3600</td>\n",
              "      <td>1.593800</td>\n",
              "      <td>1.848035</td>\n",
              "      <td>13.008700</td>\n",
              "      <td>8.640000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3650</td>\n",
              "      <td>1.638300</td>\n",
              "      <td>1.847988</td>\n",
              "      <td>13.008700</td>\n",
              "      <td>8.640000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3700</td>\n",
              "      <td>1.621100</td>\n",
              "      <td>1.847926</td>\n",
              "      <td>13.008700</td>\n",
              "      <td>8.640000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.5916913747787476\n",
            "metrics['eval_bleu']=16.0887\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.5305097103118896\n",
            "metrics['eval_bleu']=17.8726\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.5202465057373047\n",
            "metrics['eval_bleu']=17.8428\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.5240883827209473\n",
            "metrics['eval_bleu']=18.4933\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.541452407836914\n",
            "metrics['eval_bleu']=18.9772\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.5577648878097534\n",
            "metrics['eval_bleu']=18.5465\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.5578393936157227\n",
            "metrics['eval_bleu']=18.8273\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.5686310529708862\n",
            "metrics['eval_bleu']=18.677\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.5685038566589355\n",
            "metrics['eval_bleu']=18.729\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.5300099849700928\n",
            "metrics['eval_bleu']=18.5031\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.5481313467025757\n",
            "metrics['eval_bleu']=17.3715\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.743204116821289\n",
            "metrics['eval_bleu']=14.1519\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.9310286045074463\n",
            "metrics['eval_bleu']=12.285\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8779417276382446\n",
            "metrics['eval_bleu']=12.2166\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8429481983184814\n",
            "metrics['eval_bleu']=12.6688\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8318092823028564\n",
            "metrics['eval_bleu']=12.7821\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8301786184310913\n",
            "metrics['eval_bleu']=12.778\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8288711309432983\n",
            "metrics['eval_bleu']=12.7882\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.828547477722168\n",
            "metrics['eval_bleu']=12.8322\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8276057243347168\n",
            "metrics['eval_bleu']=12.9179\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8255441188812256\n",
            "metrics['eval_bleu']=13.1174\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8269972801208496\n",
            "metrics['eval_bleu']=12.9538\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8398258686065674\n",
            "metrics['eval_bleu']=12.9416\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.848159909248352\n",
            "metrics['eval_bleu']=13.0235\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.848363995552063\n",
            "metrics['eval_bleu']=13.0235\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8484046459197998\n",
            "metrics['eval_bleu']=13.0235\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.84842050075531\n",
            "metrics['eval_bleu']=13.0235\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8483580350875854\n",
            "metrics['eval_bleu']=13.0235\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8483580350875854\n",
            "metrics['eval_bleu']=13.0235\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8483693599700928\n",
            "metrics['eval_bleu']=13.0235\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8483232259750366\n",
            "metrics['eval_bleu']=13.0235\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8483283519744873\n",
            "metrics['eval_bleu']=13.0235\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8483164310455322\n",
            "metrics['eval_bleu']=13.0235\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8484009504318237\n",
            "metrics['eval_bleu']=13.0235\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.848406195640564\n",
            "metrics['eval_bleu']=13.0235\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.848364233970642\n",
            "metrics['eval_bleu']=13.0235\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8483296632766724\n",
            "metrics['eval_bleu']=13.0235\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8483296632766724\n",
            "metrics['eval_bleu']=13.0235\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8483296632766724\n",
            "metrics['eval_bleu']=13.0235\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8483041524887085\n",
            "metrics['eval_bleu']=13.0235\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8482626676559448\n",
            "metrics['eval_bleu']=13.0235\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.848254919052124\n",
            "metrics['eval_bleu']=13.0235\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.848249912261963\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8481460809707642\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8481240272521973\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8481508493423462\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8481477499008179\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8481477499008179\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8481477499008179\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8481618165969849\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.848154067993164\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.848146677017212\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8480652570724487\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8480007648468018\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8480573892593384\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8481824398040771\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.848190188407898\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.848190188407898\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8481732606887817\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8481677770614624\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8481611013412476\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8481019735336304\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8480744361877441\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8481289148330688\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8480627536773682\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.848027229309082\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8480626344680786\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8480626344680786\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8480961322784424\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8480464220046997\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8480241298675537\n",
            "metrics['eval_bleu']=13.0095\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8480345010757446\n",
            "metrics['eval_bleu']=13.0087\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8479878902435303\n",
            "metrics['eval_bleu']=13.0087\n",
            "compute_metrics. preds.shape=(1000, 20)\n",
            "dict_keys(['eval_loss', 'eval_bleu', 'eval_gen_len', 'eval_runtime', 'eval_samples_per_second', 'eval_steps_per_second', 'epoch'])\n",
            "metrics['eval_loss']=1.8479256629943848\n",
            "metrics['eval_bleu']=13.0087\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-a6eca412ee9e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         )\n\u001b[0;32m-> 1633\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1634\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1900\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1901\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1902\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2644\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2645\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2675\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2677\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2678\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2679\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1665\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mencoder_outputs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m             \u001b[0;31m# Convert encoder inputs in embeddings if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1667\u001b[0;31m             encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1668\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1072\u001b[0m                 )\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1074\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m   1075\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1076\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    691\u001b[0m             \u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcross_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m         self_attention_outputs = self.layer[0](\n\u001b[0m\u001b[1;32m    694\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    598\u001b[0m     ):\n\u001b[1;32m    599\u001b[0m         \u001b[0mnormed_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         attention_output = self.SelfAttention(\n\u001b[0m\u001b[1;32m    601\u001b[0m             \u001b[0mnormed_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;31m# get query states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 519\u001b[0;31m         \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, n_heads, seq_length, dim_per_head)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0;31m# get key/value states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "experiment.end()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "pueVwXfaaydQ",
        "outputId": "5b3bdf76-cb11-4300-fffe-03edf07e36f3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-86c369aed6fc>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'experiment' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KZzLRnEc2WBd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}