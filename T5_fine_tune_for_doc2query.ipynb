{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN7McV4qxvWqY2JTHSuu2mt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eduseiti/ia368v_dd_class_06/blob/main/T5_fine_tune_for_doc2query.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q\n",
        "!pip install evaluate -q\n",
        "!pip install ftfy -q\n",
        "!pip install sentencepiece -q\n",
        "!pip install sacrebleu -q\n"
      ],
      "metadata": {
        "id": "IyoODiJf7UDV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install comet_ml - q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVedNhQbaswU",
        "outputId": "0ed1fe15-5f60-4e0c-f3eb-0ed10afc9fe5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting comet_ml\n",
            "  Downloading comet_ml-3.32.6-py3-none-any.whl (476 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m476.9/476.9 KB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.9/dist-packages (from comet_ml) (2.27.1)\n",
            "Collecting sentry-sdk>=1.1.0\n",
            "  Downloading sentry_sdk-1.18.0-py2.py3-none-any.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 KB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websocket-client<1.4.0,>=0.55.0\n",
            "  Downloading websocket_client-1.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting simplejson\n",
            "  Downloading simplejson-3.18.4-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m136.8/136.8 KB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting semantic-version>=2.8.0\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from comet_ml) (1.16.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.9/dist-packages (from comet_ml) (1.14.1)\n",
            "Collecting wurlitzer>=1.0.2\n",
            "  Downloading wurlitzer-3.0.3-py3-none-any.whl (7.3 kB)\n",
            "Collecting dulwich!=0.20.33,>=0.20.6\n",
            "  Downloading dulwich-0.21.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (505 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m505.9/505.9 KB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting everett[ini]<3.2.0,>=1.0.1\n",
            "  Downloading everett-3.1.0-py2.py3-none-any.whl (35 kB)\n",
            "Collecting requests-toolbelt>=0.8.0\n",
            "  Downloading requests_toolbelt-0.10.1-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 KB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-box<7.0.0\n",
            "  Downloading python_box-6.1.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.9/dist-packages (from comet_ml) (4.3.3)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.9/dist-packages (from dulwich!=0.20.33,>=0.20.6->comet_ml) (1.26.15)\n",
            "Collecting configobj\n",
            "  Downloading configobj-5.0.8-py2.py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.19.3)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.9/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (22.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.18.4->comet_ml) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.18.4->comet_ml) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.18.4->comet_ml) (2022.12.7)\n",
            "Installing collected packages: everett, wurlitzer, websocket-client, simplejson, sentry-sdk, semantic-version, python-box, dulwich, configobj, requests-toolbelt, comet_ml\n",
            "Successfully installed comet_ml-3.32.6 configobj-5.0.8 dulwich-0.21.3 everett-3.1.0 python-box-6.1.0 requests-toolbelt-0.10.1 semantic-version-2.10.0 sentry-sdk-1.18.0 simplejson-3.18.4 websocket-client-1.3.3 wurlitzer-3.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "WORKING_FOLDER=\"drive/MyDrive/unicamp/ia368v_dd/aula_06\"\n",
        "\n",
        "API_KEYS_FILE=\"/content/drive/MyDrive/unicamp/ia368v_dd/api_keys_20230324.json\"\n",
        "\n",
        "TRAIN_OUTPUT_FOLDER=\"./trained_model\"\n",
        "MS_MARCO_SPLIT=\"ms_marco_tiny_data_split.pkl\"\n",
        "\n",
        "MS_MARCO_TINY_URL=\"https://storage.googleapis.com/unicamp-dl/ia368dd_2023s1/msmarco/msmarco_triples.train.tiny.tsv\"\n",
        "\n",
        "LINK_WITH_COMET=True"
      ],
      "metadata": {
        "id": "gB0nBfaiLYJC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import json\n",
        "\n",
        "import ftfy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from scipy import stats\n",
        "\n",
        "import pickle\n",
        "\n",
        "if LINK_WITH_COMET:\n",
        "    from comet_ml import Experiment"
      ],
      "metadata": {
        "id": "J1H5lQXWbFF5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive', force_remount=True)\n",
        "os.chdir(WORKING_FOLDER)"
      ],
      "metadata": {
        "id": "mFNsI1WXLtsP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57c84f1e-d30a-4fdc-dc5b-1cff3ab291cc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Link with comet-ml"
      ],
      "metadata": {
        "id": "YcuHmBPDaDLo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if LINK_WITH_COMET:\n",
        "    with open(API_KEYS_FILE) as inputFile:\n",
        "        api_keys = json.load(inputFile)\n",
        "\n",
        "    os.environ[\"COMET_API_KEY\"] = api_keys['comet_ml']\n",
        "    os.environ[\"COMET_LOG_ASSETS\"] = \"True\"\n",
        "    os.environ['COMET_MODE'] = \"ONLINE\"\n",
        "\n",
        "    Experiment(api_key=api_keys['comet_ml'], \n",
        "            project_name=\"causal-language-model-fine-tuning\",\n",
        "            workspace=\"eduseiti\")    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mXxEFPRTaBf4",
        "outputId": "97fc3306-339b-496a-a528-5dd091ce5943"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "COMET ERROR: Failed to calculate active processors count. Fall back to default CPU count 1\n",
            "COMET INFO: Couldn't find a Git repository in '/content/drive/MyDrive/unicamp/ia368v_dd/aula_06' nor in any parent directory. You can override where Comet is looking for a Git Patch by setting the configuration `COMET_GIT_DIRECTORY`\n",
            "COMET INFO: Experiment is live on comet.com https://www.comet.com/eduseiti/causal-language-model-fine-tuning/77781656225a47aeb40639bb1c96c6ae\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (AutoTokenizer, \n",
        "                          AutoModelForSeq2SeqLM, \n",
        "                          Seq2SeqTrainer,\n",
        "                          Seq2SeqTrainingArguments,\n",
        "                          TrainerCallback, \n",
        "                          get_cosine_with_hard_restarts_schedule_with_warmup,\n",
        "                          DataCollatorForSeq2Seq,\n",
        "                          T5Tokenizer, \n",
        "                          T5Model\n",
        "                          )\n",
        "\n",
        "import torch\n",
        "\n",
        "import evaluate"
      ],
      "metadata": {
        "id": "4IGkkxJCjxef"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth', None)"
      ],
      "metadata": {
        "id": "wgP6ie7r_uqD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the MS MARCO data split, if available"
      ],
      "metadata": {
        "id": "1Gtj85x8yiCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if os.path.exists(MS_MARCO_SPLIT):\n",
        "    with open(MS_MARCO_SPLIT, \"rb\") as inputFile:\n",
        "        ms_marco_data = pickle.load(inputFile)\n",
        "\n",
        "    train_df = ms_marco_data['train']\n",
        "    validation_df = ms_marco_data['validation']\n",
        "else:\n",
        "    print(\"Need to import and fix the training dataset...\")"
      ],
      "metadata": {
        "id": "g_XrVNA7yl5-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import and fix training dataset"
      ],
      "metadata": {
        "id": "oR10gOwEh83h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not 'train_df' in locals():\n",
        "    if not os.path.exists(os.path.basename(MS_MARCO_TINY_URL)):\n",
        "        !wget {MS_MARCO_TINY_URL}\n",
        "    else:\n",
        "        print(\"Training dataset already downloaded...\")\n",
        "\n",
        "    ms_df = pd.read_csv(os.path.basename(MS_MARCO_TINY_URL), sep=\"\\t\", header=None, names=['topic', 'positive', 'negative'])\n",
        "    pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "    display(ms_df.head())\n",
        "\n",
        "    ms_df['positive'] = ms_df['positive'].apply(lambda text: ftfy.fix_text(text))\n",
        "    ms_df = ms_df.drop('negative', axis=1)\n",
        "\n",
        "else:\n",
        "    print(\"Data split has already been loaded...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEB5xlBIYL1Q",
        "outputId": "0420334f-1914-4651-9cfe-d915f7172a8e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data split has already been loaded...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split evaluation part"
      ],
      "metadata": {
        "id": "DvWpFpGpwaFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not 'train_df' in locals():\n",
        "    print(\"ms_df.shape={}\".format(ms_df.shape))\n",
        "\n",
        "    split_entries = np.random.choice(list(range(ms_df.shape[0])), 1000, replace=False)\n",
        "\n",
        "    train_df = ms_df.iloc[np.setdiff1d(list(range(ms_df.shape[0])), split_entries)].reset_index(drop=True)\n",
        "    validation_df = ms_df.iloc[split_entries].reset_index(drop=True)\n",
        "    \n",
        "    print(\"train_df.shape={}\".format(train_df.shape))\n",
        "    print(\"validation_df.shape={}\".format(validation_df.shape))\n",
        "    \n",
        "    with open(MS_MARCO_SPLIT, \"wb\") as outputFile:\n",
        "        pickle.dump({'train': train_df, \n",
        "                    'validation': validation_df}, outputFile, pickle.HIGHEST_PROTOCOL)\n",
        "        \n",
        "else:\n",
        "    print(\"Data split has already been loaded...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVj26xQNwaBm",
        "outputId": "19f3b512-a2cb-49cc-e691-ce4886323be0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data split has already been loaded...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fine tune the T5-base model for the query generation"
      ],
      "metadata": {
        "id": "ONzoumZjc5W6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Doc2queryFinetuning(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, ms_df, tokenizer):\n",
        "\n",
        "        self.tokenized_topics = tokenizer(ms_df['topic'].tolist(), return_length=True)\n",
        "        self.tokenized_passage = tokenizer(ms_df['positive'].tolist(), return_length=True)\n",
        "\n",
        "        print(\"Topics tokens size stats:\\n{}\\n\".format(stats.describe(self.tokenized_topics['length'])))\n",
        "        print(\"Passages tokens size stats:\\n{}\\n\".format(stats.describe(self.tokenized_passage['length'])))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_topics['input_ids'])\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return {'input_ids': self.tokenized_passage['input_ids'][index],\n",
        "                'attention_mask': self.tokenized_passage['attention_mask'][index],\n",
        "                'labels': self.tokenized_topics['input_ids'][index]}"
      ],
      "metadata": {
        "id": "4X2_jX_tw3Vr"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTrainerCallback(TrainerCallback):\n",
        "\n",
        "    def __init__(self, best_validation_yet=99999, model=None) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.best_validation_metric = best_validation_yet\n",
        "        self.model = model\n",
        "\n",
        "\n",
        "    def on_evaluate(self, args, state, control, model=None, metrics=None, **kwargs):\n",
        "        print(metrics.keys())\n",
        "\n",
        "        print(\"metrics['eval_loss']={}\".format(metrics['eval_loss']))\n",
        "        print(\"metrics['eval_bleu']={}\".format(metrics['eval_bleu']))\n",
        "\n",
        "\n",
        "        if metrics['eval_bleu'] < self.best_validation_metric:\n",
        "            self.model.save_pretrained(os.path.join(TRAIN_OUTPUT_FOLDER, \n",
        "                                                    \"checkpoint-{}-{:.4f}\".format(state.global_step,\n",
        "                                                                                  metrics['eval_bleu'])))\n",
        "            self.best_validation_metric = metrics['eval_bleu']"
      ],
      "metadata": {
        "id": "xiR4Dm69w3QB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare T5 model"
      ],
      "metadata": {
        "id": "Cyv1yI_xw2hm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-base\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhGbxDBGkCXf",
        "outputId": "5ebce202-047a-473d-a8f8-7cb9d095e275"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/models/t5/tokenization_t5_fast.py:155: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.model_max_length"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hpe6QbnI6a81",
        "outputId": "82d6a203-6517-4b67-9aab-766bcd43b6b9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "512"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")"
      ],
      "metadata": {
        "id": "L3YKKvEUlpkM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the datasets"
      ],
      "metadata": {
        "id": "Cqq9J6BkzqUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = Doc2queryFinetuning(train_df, tokenizer)\n",
        "eval_dataset = Doc2queryFinetuning(validation_df, tokenizer)"
      ],
      "metadata": {
        "id": "kX75h3oCw3Kw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d20ba08e-8a34-497a-ee45-ff094f1aaf5c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topics tokens size stats:\n",
            "DescribeResult(nobs=10000, minmax=(3, 57), mean=9.5105, variance=11.948684618461847, skewness=1.7547439898740915, kurtosis=10.741828171750589)\n",
            "\n",
            "Passages tokens size stats:\n",
            "DescribeResult(nobs=10000, minmax=(14, 326), mean=86.9099, variance=1264.7692589158914, skewness=1.140892273381897, kurtosis=1.6057605062545974)\n",
            "\n",
            "Topics tokens size stats:\n",
            "DescribeResult(nobs=1000, minmax=(3, 29), mean=9.434, variance=11.218862862862863, skewness=1.3393629730350802, kurtosis=4.007153813445774)\n",
            "\n",
            "Passages tokens size stats:\n",
            "DescribeResult(nobs=1000, minmax=(16, 261), mean=87.08, variance=1265.3289289289291, skewness=1.1804618158884206, kurtosis=1.6154309574375878)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This part was taken from the [`run_translation.py`](https://github.com/huggingface/transformers/blob/main/examples/pytorch/translation/run_translation.py) script."
      ],
      "metadata": {
        "id": "Y9VXQsi5BBWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metric = evaluate.load(\"sacrebleu\")"
      ],
      "metadata": {
        "id": "xRC29Hizk56U"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "\n",
        "    return preds, labels"
      ],
      "metadata": {
        "id": "Nj8a-g2jBy5G"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_preds):\n",
        "\n",
        "    preds, labels = eval_preds\n",
        "\n",
        "    # print(\"len(preds)={}\".format(len(preds)))\n",
        "\n",
        "    # for i in range(len(preds)):\n",
        "    #     print(\"len(preds[{}])={}\".format(i, len(preds[i])))\n",
        "    #     print(\"preds[{}].shape={}\".format(i, preds[i].shape))\n",
        "    #     print(\"preds[i]={}\".format(preds[i]))\n",
        "\n",
        "\n",
        "    if isinstance(preds, tuple):\n",
        "        preds = preds[0]\n",
        "\n",
        "    print(\"compute_metrics. preds.shape={}\".format(preds.shape))\n",
        "\n",
        "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Some simple post-processing\n",
        "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
        "\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
        "    result = {\"bleu\": result[\"score\"]}\n",
        "\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
        "    \n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "    \n",
        "    result = {k: round(v, 4) for k, v in result.items()}\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "doB4B3SVA4s_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size=26\n",
        "epochs=100"
      ],
      "metadata": {
        "id": "kT5rUueI658D"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_params = Seq2SeqTrainingArguments(output_dir=TRAIN_OUTPUT_FOLDER,\n",
        "                                           num_train_epochs=epochs,\n",
        "                                           per_device_train_batch_size=batch_size,\n",
        "                                           per_device_eval_batch_size=batch_size,\n",
        "                                           gradient_accumulation_steps=8,\n",
        "                                           evaluation_strategy='steps',\n",
        "                                           eval_steps=100,\n",
        "                                           save_strategy='steps',\n",
        "                                           save_steps=1000,\n",
        "                                           logging_strategy='steps',\n",
        "                                           logging_steps=10,\n",
        "                                           save_total_limit=10,\n",
        "                                           # report_to='comet_ml',\n",
        "                                           # dataloader_num_workers=2,\n",
        "                                           dataloader_pin_memory=True,\n",
        "                                           predict_with_generate=True,\n",
        "                                           fp16=True)"
      ],
      "metadata": {
        "id": "99L0Mq_p7s4M"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_pad_token_id = -100\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=label_pad_token_id,\n",
        "    pad_to_multiple_of=8 if training_params.fp16 else None,\n",
        ")"
      ],
      "metadata": {
        "id": "FsmNhge3AIRP"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_callback = CustomTrainerCallback(best_validation_yet=999999, \n",
        "                                         model=model)"
      ],
      "metadata": {
        "id": "phSscTZI3hO4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_training_steps = epochs * len(train_dataset)\n",
        "\n",
        "optimzer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-3)\n",
        "scheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimzer, \n",
        "                                                               num_training_steps * 0.1, \n",
        "                                                               num_training_steps, \n",
        "                                                               num_cycles=80)"
      ],
      "metadata": {
        "id": "8y9rWOcRbZoV"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Seq2SeqTrainer(model=model,\n",
        "                         args=training_params,\n",
        "                         train_dataset=train_dataset,\n",
        "                         eval_dataset=eval_dataset,\n",
        "                         data_collator=data_collator,\n",
        "                         callbacks=[trainer_callback],\n",
        "                         optimizers=(optimzer, scheduler),\n",
        "                         tokenizer=tokenizer,\n",
        "                         compute_metrics=compute_metrics\n",
        "                         )"
      ],
      "metadata": {
        "id": "77Sgz4OB8nMp"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_result = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "mefCKkW60Ko0",
        "outputId": "b81c0ab1-1f23-41b3-b699-bda519a10ab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "COMET INFO: ---------------------------------------------------------------------------------------\n",
            "COMET INFO: Comet.ml Experiment Summary\n",
            "COMET INFO: ---------------------------------------------------------------------------------------\n",
            "COMET INFO:   Data:\n",
            "COMET INFO:     display_summary_level : 1\n",
            "COMET INFO:     url                   : https://www.comet.com/eduseiti/causal-language-model-fine-tuning/77781656225a47aeb40639bb1c96c6ae\n",
            "COMET INFO:   Uploads:\n",
            "COMET INFO:     environment details : 1\n",
            "COMET INFO:     filename            : 1\n",
            "COMET INFO:     installed packages  : 1\n",
            "COMET INFO:     notebook            : 2\n",
            "COMET INFO:     os packages         : 1\n",
            "COMET INFO:     source_code         : 1\n",
            "COMET INFO: \n",
            "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
            "COMET ERROR: Failed to calculate active processors count. Fall back to default CPU count 1\n",
            "COMET INFO: Couldn't find a Git repository in '/content/drive/MyDrive/unicamp/ia368v_dd/aula_06' nor in any parent directory. You can override where Comet is looking for a Git Patch by setting the configuration `COMET_GIT_DIRECTORY`\n",
            "COMET INFO: Experiment is live on comet.com https://www.comet.com/eduseiti/huggingface/f13f994283fc462ba1757eab11a66cba\n",
            "\n",
            "COMET ERROR: Failed to extract scalar from SummaryWriter.add_hparams()\n",
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='82' max='4800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  82/4800 04:19 < 4:15:27, 0.31 it/s, Epoch 1.68/100]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "experiment.end()"
      ],
      "metadata": {
        "id": "pueVwXfaaydQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}